{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.df_loader import (\n",
    "    load_adult_df,\n",
    "    load_compas_df,\n",
    "    load_german_df,\n",
    "    load_diabetes_df,\n",
    "    load_breast_cancer_df,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.preprocessing import preprocess_df\n",
    "from utils.models import (\n",
    "    train_models,\n",
    "    evaluation_test,\n",
    "    save_models,\n",
    "    load_models,\n",
    ")\n",
    "\n",
    "from art.attacks.evasion import DeepFool, CarliniL0Method, CarliniLInfMethod, CarliniL2Method, BoundaryAttack, LowProFool\n",
    "from art.estimators.classification import SklearnClassifier, KerasClassifier\n",
    "\n",
    "from utils.preprocessing import DfInfo\n",
    "from utils.preprocessing import inverse_dummy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "seed = 42\n",
    "# tf.random.set_seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None # suppress \"SettingWithCopyWarning\" warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.10.0\n",
      "Eager execution enabled:  False\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MODEL = False # (1)True(2)False\n",
    "RUN_ART = True\n",
    "num_instances = 64 # (1)&(2) 20\n",
    "\n",
    "if RUN_ART:\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Eager execution enabled: \", tf.executing_eagerly())  # False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Name: [german]\n"
     ]
    }
   ],
   "source": [
    "#### Select dataset ####\n",
    "for dataset_name in [\n",
    "    # \"adult\",\n",
    "    \"german\",\n",
    "    #  \"compas\",\n",
    "    # \"diabetes\",\n",
    "    # \"breast_cancer\",\n",
    "]:  # [adult, german, compas]\n",
    "    print(f\"Dataset Name: [{dataset_name}]\")\n",
    "    if dataset_name == \"adult\":\n",
    "        dataset_loading_fn = load_adult_df\n",
    "    elif dataset_name == \"german\":\n",
    "        dataset_loading_fn = load_german_df\n",
    "    elif dataset_name == \"compas\":\n",
    "        dataset_loading_fn = load_compas_df\n",
    "    elif dataset_name == \"diabetes\":\n",
    "        dataset_loading_fn = load_diabetes_df\n",
    "    elif dataset_name == \"breast_cancer\":\n",
    "        dataset_loading_fn = load_breast_cancer_df\n",
    "    else:\n",
    "        raise Exception(\"Unsupported dataset\")\n",
    "\n",
    "    df_info = preprocess_df(dataset_loading_fn)\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_info.dummy_df, train_size=0.8, random_state=seed, shuffle=True\n",
    "    )\n",
    "    X_train = np.array(train_df[df_info.ohe_feature_names])\n",
    "    y_train = np.array(train_df[df_info.target_name])\n",
    "    X_test = np.array(test_df[df_info.ohe_feature_names])\n",
    "    y_test = np.array(test_df[df_info.target_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\n10599070\\Miniconda3\\envs\\xai\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "### Load models\n",
    "models = load_models(X_train.shape[-1], dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances = 64 # (1)&(2) 20\n",
    "\n",
    "X_test_re=X_test[0:num_instances]\n",
    "y_test_re=y_test[0:num_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m attack \u001b[39m=\u001b[39m CarliniL0Method(classifier\u001b[39m=\u001b[39mKerasClassifier(models[\u001b[39m'\u001b[39m\u001b[39mnn_2\u001b[39m\u001b[39m'\u001b[39m], clip_values\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)), verbose\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m x_adv \u001b[39m=\u001b[39m attack\u001b[39m.\u001b[39mgenerate(X_test_re)\n",
      "File \u001b[1;32mc:\\Users\\n10599070\\Miniconda3\\envs\\xai\\lib\\site-packages\\art\\attacks\\evasion\\carlini.py:1077\u001b[0m, in \u001b[0;36mCarliniL0Method.generate\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[39m# compute gradient:\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCompute loss gradient\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1077\u001b[0m perturbation_tanh \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_gradient(\n\u001b[0;32m   1078\u001b[0m     z_logits[active],\n\u001b[0;32m   1079\u001b[0m     y_batch[active],\n\u001b[0;32m   1080\u001b[0m     x_batch[active],\n\u001b[0;32m   1081\u001b[0m     x_adv_batch[active],\n\u001b[0;32m   1082\u001b[0m     x_adv_batch_tanh[active],\n\u001b[0;32m   1083\u001b[0m     c_current[active],\n\u001b[0;32m   1084\u001b[0m     clip_min,\n\u001b[0;32m   1085\u001b[0m     clip_max,\n\u001b[0;32m   1086\u001b[0m )\n\u001b[0;32m   1088\u001b[0m \u001b[39m# perform line search to optimize perturbation\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[39m# first, halve the learning rate until perturbation actually decreases the loss:\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m prev_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\n10599070\\Miniconda3\\envs\\xai\\lib\\site-packages\\art\\attacks\\evasion\\carlini.py:214\u001b[0m, in \u001b[0;36mCarliniL2Method._loss_gradient\u001b[1;34m(self, z_logits, target, x, x_adv, x_adv_tanh, c_weight, clip_min, clip_max)\u001b[0m\n\u001b[0;32m    208\u001b[0m     i_add \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(target, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    209\u001b[0m     i_sub \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\n\u001b[0;32m    210\u001b[0m         z_logits \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m target) \u001b[39m+\u001b[39m (np\u001b[39m.\u001b[39mmin(z_logits, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)[:, np\u001b[39m.\u001b[39mnewaxis] \u001b[39m*\u001b[39m target,\n\u001b[0;32m    211\u001b[0m         axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    212\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m loss_gradient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mclass_gradient(x_adv, label\u001b[39m=\u001b[39;49mi_add)\n\u001b[0;32m    215\u001b[0m loss_gradient \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mclass_gradient(x_adv, label\u001b[39m=\u001b[39mi_sub)\n\u001b[0;32m    216\u001b[0m loss_gradient \u001b[39m=\u001b[39m loss_gradient\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\n10599070\\Miniconda3\\envs\\xai\\lib\\site-packages\\art\\estimators\\classification\\keras.py:515\u001b[0m, in \u001b[0;36mKerasClassifier.class_gradient\u001b[1;34m(self, x, label, training_mode, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m grad_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_gradients_idx[u_l]\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m grad_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     gradients_list\u001b[39m.\u001b[39mappend(grad_fn([x_preprocessed, \u001b[39mint\u001b[39;49m(training_mode)]))\n\u001b[0;32m    516\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mClass gradient operation is not defined.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\n10599070\\Miniconda3\\envs\\xai\\lib\\site-packages\\keras\\backend.py:4577\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4567\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callable_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   4569\u001b[0m     \u001b[39mor\u001b[39;00m feed_arrays \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_arrays\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4573\u001b[0m     \u001b[39mor\u001b[39;00m session \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session\n\u001b[0;32m   4574\u001b[0m ):\n\u001b[0;32m   4575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4577\u001b[0m fetched \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callable_fn(\u001b[39m*\u001b[39;49marray_vals, run_metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_metadata)\n\u001b[0;32m   4578\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches) :])\n\u001b[0;32m   4579\u001b[0m output_structure \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4581\u001b[0m     fetched[: \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)],\n\u001b[0;32m   4582\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   4583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\n10599070\\Miniconda3\\envs\\xai\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1480\u001b[0m   run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1481\u001b[0m   ret \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39;49mTF_SessionRunCallable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_session,\n\u001b[0;32m   1482\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, args,\n\u001b[0;32m   1483\u001b[0m                                          run_metadata_ptr)\n\u001b[0;32m   1484\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1485\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "attack = CarliniL0Method(classifier=KerasClassifier(models['nn_2'], clip_values=(0,1)), verbose= True, batch_size=64)\n",
    "x_adv = attack.generate(X_test_re) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = models['nn_2'].predict(x_adv).argmax(axis=1).flatten().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(predictions, y_test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "350746cdd709c967d80ca4cb0f1d2cbf04d079be136a05ad7342c703ce6b7c0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
