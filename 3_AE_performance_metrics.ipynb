{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils.preprocessing import preprocess_df\n",
    "from utils.df_loader import load_adult_df, load_compas_df, load_german_df, load_diabetes_df, load_breast_cancer_df\n",
    "from utils.evaluation import get_evaluations, EvaluationMatrix\n",
    "\n",
    "from utils.load import load_result_from_csv, load_datapoints_from_npy\n",
    "from utils.models import load_models\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.options.mode.chained_assignment = None # suppress \"SettingWithCopyWarning\" warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "        \"adult\",\n",
    "        # \"german\",\n",
    "        # \"compas\",\n",
    "        # \"diabetes\",\n",
    "        # \"breast_cancer\",\n",
    "        ]\n",
    "\n",
    "models = [\"dt\",\"gbc\",\"lr\",\"svc\",\"nn_2\"] # \"dt\",\"gbc\",\"lr\",\"svc\",\n",
    "\n",
    "\n",
    "attack_list = [\n",
    "        'deepfool', \n",
    "        # 'carlini_l_2', 'carlini_l_inf', \n",
    "        # 'lowprofool_l_2', 'lowprofool_l_inf', \n",
    "        # 'boundary', \n",
    "        # 'hopskipjump_l_2', 'hopskipjump_l_inf'\n",
    "        ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name in datasets:\n",
    "#     for attack in attack_list:\n",
    "#         folder_name = f\"{attack}_{dataset_name}\"\n",
    "\n",
    "#         ## check if the folder exist\n",
    "\n",
    "#         if os.path.isdir(f'./results/{folder_name}'):\n",
    "#             for model_name in models:\n",
    "\n",
    "#                 dfs = []\n",
    "#                 file_name = f'{folder_name}_{model_name}_result.csv'\n",
    "#                 destination_path = f'./results/{folder_name}/{file_name}'\n",
    "\n",
    "#                 if os.path.isfile(f'./results/{folder_name}/{folder_name}_{model_name}_result_1.csv'):\n",
    "#                     for i in range(0,10):\n",
    "#                         dataset_path = (\n",
    "#                             f\"{attack}_{dataset_name}_{model_name}_result_{i}.csv\"\n",
    "#                         )\n",
    "#                         dfs.append(pd.read_csv(f\"./results/{folder_name}/{dataset_path}\"))\n",
    "\n",
    "#                     ### Combine dfs\n",
    "#                     complete_df = pd.DataFrame([], columns=dfs[0].columns)\n",
    "#                     for l in range(len(dfs[0])):\n",
    "#                         for df in dfs:\n",
    "#                             complete_df = complete_df.append(df.iloc[l : l + 1])\n",
    "\n",
    "#                     ### Save dfs\n",
    "#                     complete_df.to_csv(destination_path)\n",
    "#                     print(f\"Have saved combined sheet to {destination_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check wheather white-box attack output same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_ndarrays_same(ndarrays):\n",
    "  # Create an empty matrix of size len(ndarrays) x len(ndarrays)\n",
    "  results_matrix = np.empty((len(ndarrays), len(ndarrays)))\n",
    "\n",
    "  # Iterate through each pair of ndarrays and check if they are the same\n",
    "  for i in range(len(ndarrays)):\n",
    "    for j in range(len(ndarrays)):\n",
    "      if i == j:\n",
    "        # If the indices are the same, mark it as True in the results matrix\n",
    "        results_matrix[i][j] = True\n",
    "      else:\n",
    "        # Compare the ndarrays using the numpy.array_equal function\n",
    "        results_matrix[i][j] = np.array_equal(ndarrays[i], ndarrays[j])\n",
    "  return results_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult - deepfool - dt\n",
      "1.0\n",
      "adult - deepfool - gbc\n",
      "1.0\n",
      "adult - deepfool - lr\n",
      "1.0\n",
      "adult - deepfool - svc\n",
      "1.0\n",
      "adult - deepfool - nn_2\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    for attack in attack_list:\n",
    "        for model_name in models:\n",
    "            ndarrays = []\n",
    "            for running_times in range(0,10):\n",
    "                ndarrays.append(load_datapoints_from_npy(attack, dataset_name, model_name, running_times, adv=True))\n",
    "            print(f'{dataset_name} - {attack} - {model_name}')\n",
    "            print(are_ndarrays_same(ndarrays).min())\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loading_fn(dataset_name):\n",
    "    if dataset_name == 'adult':\n",
    "        dataset_loading_fn = load_adult_df\n",
    "    elif dataset_name == 'german':\n",
    "        dataset_loading_fn = load_german_df\n",
    "    elif dataset_name == 'compas':\n",
    "        dataset_loading_fn = load_compas_df\n",
    "    elif dataset_name == 'diabetes':\n",
    "        dataset_loading_fn = load_diabetes_df\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        dataset_loading_fn = load_breast_cancer_df\n",
    "    else:\n",
    "        raise Exception(\"Unsupported dataset\")\n",
    "    return dataset_loading_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Select dataset ####\n",
    "\n",
    "# all_metric = {}\n",
    "\n",
    "# for dataset_name in datasets:\n",
    "#     all_metric[dataset_name]={}    \n",
    "\n",
    "#     df_info = preprocess_df(get_loading_fn(dataset_name))\n",
    "#     for attack in attack_list:\n",
    "#         all_metric[dataset_name][attack]={}\n",
    "\n",
    "#         folder_name = f'{attack}_{dataset_name}'\n",
    "#         models_list = load_models(df_info.dummy_df.shape[-1], dataset_name)\n",
    "#         for model_name in models:\n",
    "\n",
    "#             file_name = f'{folder_name}_{model_name}_result.csv'\n",
    "#             result_path = f'./results/{folder_name}/{file_name}'\n",
    "#             if os.path.isfile(result_path):\n",
    "#                 result_df = pd.read_csv(result_path)\n",
    "#                 evaluation_df, metric = get_evaluations(result_df, \n",
    "#                     df_info, \n",
    "#                     matrix = [\n",
    "#                         EvaluationMatrix.L1, \n",
    "#                         EvaluationMatrix.L2, \n",
    "#                         EvaluationMatrix.Linf,\n",
    "#                         EvaluationMatrix.Sparsity, \n",
    "#                         # EvaluationMatrix.Realistic, \n",
    "#                         EvaluationMatrix.MAD, \n",
    "#                         EvaluationMatrix.Mahalanobis,\n",
    "#                         # EvaluationMatrix.Perturbation_Sensitivity,\n",
    "#                         EvaluationMatrix.Neighbour_Distance,\n",
    "#                         ],\n",
    "#                     models = models_list,\n",
    "#                     model_name=model_name)\n",
    "                \n",
    "#                 all_metric[dataset_name][attack][model_name] = metric\n",
    "\n",
    "#                 csv_save_result_path = f'results/{folder_name}/eval_{file_name}'\n",
    "#                 evaluation_df.to_csv(csv_save_result_path)\n",
    "#                 print(f\"Have saved file to {csv_save_result_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dic_from_metric(all_metric):\n",
    "\n",
    "    dataset_arr = []\n",
    "    attack_arr = []\n",
    "    model_arr = []\n",
    "    metric_arr = []\n",
    "    value_arr = []\n",
    "\n",
    "    for dataset, dic1 in all_metric.items():\n",
    "        for attack, dic2 in dic1.items():\n",
    "            for model, dic3 in dic2.items():\n",
    "                for metric, value in dic3.items():\n",
    "                    dataset_arr.append(dataset)\n",
    "                    attack_arr.append(attack)\n",
    "                    model_arr.append(model)\n",
    "                    metric_arr.append(metric)\n",
    "                    value_arr.append(value)\n",
    "\n",
    "    table = {\n",
    "            'Dataset': dataset_arr,\n",
    "            'Attack': attack_arr,\n",
    "            'Model': model_arr,\n",
    "            'Metric': metric_arr,\n",
    "            'Value': value_arr,\n",
    "        }\n",
    "\n",
    "    return table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have saved file to results/deepfool_adult/eval_deepfool_adult_lr_result_0.csv\n",
      "Have saved file to results/deepfool_adult/eval_deepfool_adult_svc_result_0.csv\n",
      "Have saved file to results/deepfool_adult/eval_deepfool_adult_nn_2_result_0.csv\n"
     ]
    }
   ],
   "source": [
    "#### Select dataset ####\n",
    "\n",
    "all_metric = {}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    all_metric[dataset_name]={}\n",
    "\n",
    "    df_info = preprocess_df(get_loading_fn(dataset_name))\n",
    "    for attack in attack_list:\n",
    "        all_metric[dataset_name][attack]={}\n",
    "\n",
    "        folder_name = f'{attack}_{dataset_name}'\n",
    "        for model_name in models:\n",
    "\n",
    "            file_name = f'{folder_name}_{model_name}_result_0.csv'\n",
    "            result_path = f'./results/{folder_name}/{file_name}'\n",
    "            if os.path.isfile(result_path):\n",
    "                result_df = pd.read_csv(result_path)\n",
    "                evaluation_df, metric = get_evaluations(result_df=result_df, \n",
    "                    df_info=df_info, \n",
    "                    matrix = [\n",
    "                        EvaluationMatrix.L1, \n",
    "                        EvaluationMatrix.L2, \n",
    "                        EvaluationMatrix.Linf,\n",
    "                        EvaluationMatrix.Sparsity, \n",
    "                        # EvaluationMatrix.Realistic, \n",
    "                        EvaluationMatrix.MAD, \n",
    "                        EvaluationMatrix.Mahalanobis,\n",
    "                        # EvaluationMatrix.Perturbation_Sensitivity,\n",
    "                        EvaluationMatrix.Neighbour_Distance,\n",
    "                        ])\n",
    "                \n",
    "                all_metric[dataset_name][attack][model_name] = metric\n",
    "\n",
    "                csv_save_result_path = f'results/{folder_name}/eval_{file_name}'\n",
    "                evaluation_df.to_csv(csv_save_result_path)\n",
    "                print(f\"Have saved file to {csv_save_result_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_results = pd.DataFrame.from_dict(get_dic_from_metric(all_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_results.to_csv(f\"./results/deepfool_adult_table.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Select dataset ####\n",
    "# for dataset_name in datasets: \n",
    "#     All_Results[dataset_name] = {}\n",
    "#     All_Datapoints[dataset_name] = {}\n",
    "\n",
    "#     for model in models:\n",
    "#         All_Results[dataset_name][model] = {}\n",
    "#         All_Datapoints[dataset_name][model] = {}\n",
    "\n",
    "#         for attack in attack_list:\n",
    "\n",
    "#             All_Results[dataset_name][model][attack] = load_result_from_csv(attack, dataset_name, model)\n",
    "\n",
    "#             All_Datapoints[dataset_name][model][attack] = {\n",
    "#                 \"original\": load_datapoints_from_npy(attack, dataset_name, model, False),\n",
    "#                 \"adv\": load_datapoints_from_npy(attack, dataset_name, model, True),\n",
    "#             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All_Metrics = {}\n",
    "\n",
    "# for dataset_name in datasets: \n",
    "#     All_Metrics[dataset_name] = {}\n",
    "#     for model in models: \n",
    "#         All_Metrics[dataset_name][model] = {}\n",
    "#         for attack in attack_list:\n",
    "\n",
    "#             if All_Results[dataset_name][model][attack] is not None:\n",
    "#                 if All_Results[dataset_name][model][attack][\"original\"] is not None:\n",
    "#                     if All_Results[dataset_name][model][attack][\"adv\"] is not None:\n",
    "#                         All_Metrics[dataset_name][model][attack] = metric_generator(\n",
    "#                             All_Results[dataset_name][model][attack],\n",
    "#                             All_Results[dataset_name][model][attack][\"original\"],\n",
    "#                             All_Results[dataset_name][model][attack][\"adv\"]\n",
    "#                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "350746cdd709c967d80ca4cb0f1d2cbf04d079be136a05ad7342c703ce6b7c0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
